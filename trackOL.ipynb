{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \r\n",
    "import torch\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import layers\r\n",
    "from torchvision.io.image import read_image, ImageReadMode\r\n",
    "from params import *\r\n",
    "import image\r\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\r\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt(gt_dir):\r\n",
    "    gts = list()\r\n",
    "    with open(gt_dir, 'r') as labels:\r\n",
    "        for line in labels:\r\n",
    "            temp = list()\r\n",
    "            for num in line.split(','):\r\n",
    "                temp.append(float(num))\r\n",
    "            gts.append(temp)\r\n",
    "    return gts\r\n",
    "\r\n",
    "def creat_label(label_shape):\r\n",
    "    h, w = label_shape\r\n",
    "    gaussian_func = MultivariateNormal((h/2, w/2), 2 * torch.eye(2))\r\n",
    "    label = tensor.zeros(h, w)\r\n",
    "    \r\n",
    "    for x in range(h):\r\n",
    "        for y in range(w):\r\n",
    "            label[x][y] = np.exp(gaussian_func.log_prob((x, y)))\r\n",
    "    \r\n",
    "    return label\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_m = layers.VGG_M()\r\n",
    "vgg_m = vgg_m.load_state_dict(torch.load('./States/vgg_param.pth'))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\r\n",
    "\r\n",
    "scales =  scale_factor ** np.linspace(np.floor((1 -scale_num)/2), np.floor((scale_num - 1)/2), num = scale_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\r\n",
    "from torch.autograd.functional import jacobian, hessian\r\n",
    "def subgrid_detect(raw_score, context_width, context_height, x, y):\r\n",
    "    M, N = raw_score.shape\r\n",
    "    def func(input):\r\n",
    "        out = torch.zeros(1, dtype = torch.complex64)\r\n",
    "        for m in range(M):\r\n",
    "            for n in range(N):\r\n",
    "                out += raw_score[m][n] * torch.exp(1j * math.pi * 2 * (m/M * input[0] + n/N*input[1]))\r\n",
    "\r\n",
    "        return out\r\n",
    "        \r\n",
    "    current = torch.tensor([x, y])\r\n",
    "    while(True):\r\n",
    "        jaco = jacobian(func, last)\r\n",
    "        hess = hessian(func, last)\r\n",
    "        inverse_hess = torch.inverse(hess)\r\n",
    "        last = torch.clone(current)\r\n",
    "        current = last - torch.squeeze(torch.matmul(inverse_hess, jaco))\r\n",
    "        if(torch.dist(current, last) < 1e-2):\r\n",
    "            break\r\n",
    "    \r\n",
    "    dis_y, dis_x = torch.tensor([M/2, N/2]) - current\r\n",
    "    dis_y = dis_y * context_height / M\r\n",
    "    dis_x = dis_x * context_width / N\r\n",
    "    return dis_x, dis_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "padding_frame() missing 1 required positional argument: 'height'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6ce90cb90c42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageReadMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mcr_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre_sz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mcr_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcr_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mconv1_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv2_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv3_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv4_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv5_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg_m\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcr_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\Desktop\\毕业设计\\code\\DeepDCF\\image.py\u001b[0m in \u001b[0;36mcr\u001b[1;34m(img, center_x, center_y, width, height, re_sz)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mresized_img\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mmean_rgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mresized_img\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mmean_rgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mresized_img\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mmean_rgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mres_imgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresized_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: padding_frame() missing 1 required positional argument: 'height'"
     ]
    }
   ],
   "source": [
    "\r\n",
    "for video in track_videos:\r\n",
    "    imgs_dir = os.path.join(dataset_dir, video)\r\n",
    "    gt_dir = os.path.join(imgs_dir, gt_file)\r\n",
    "    img_dir = os.path.join(imgs_dir, f'{1:08d}.png')\r\n",
    "    gts = get_gt(gt_dir)\r\n",
    "    cnt = len(gts)\r\n",
    "\r\n",
    "    pos_x, pos_y, width, height = gts[0]\r\n",
    "    center_x, center_y = width/2, height/2\r\n",
    "    context_width, context_height = context_factor * width, context_factor*height\r\n",
    "\r\n",
    "    img = read_image(img_dir, ImageReadMode.RGB).to(dtype = torch.float32)\r\n",
    "    cr_img = image.cr(img, center_x, center_y, context_width, context_height, re_sz)\r\n",
    "    cr_img = torch.unsqueeze(cr_img, dim = 0)\r\n",
    "    conv1_img, conv2_img, conv3_img, conv4_img, conv5_img = vgg_m(cr_img)\r\n",
    "    \r\n",
    "    x_DCF = layers.DCF(img, create_label(img.shape[-2:]))\r\n",
    "    conv1_DCF = layers.DCF(conv1_img, create_label(conv1_img.shape[-2:]))\r\n",
    "    conv2_DCF = layers.DCF(conv2_img, create_label(conv2_img.shape[-2:]))\r\n",
    "    conv3_DCF = layers.DCF(conv3_img, create_label(conv3_img.shape[-2:]))\r\n",
    "    conv4_DCF = layers.DCF(conv4_img, create_label(conv4_img.shape[-2:]))\r\n",
    "    conv5_DCF = layers.DCF(conv5_img, create_label(conv5_img.shape[-2:]))\r\n",
    "    DCFS = [x_DCF, conv1_DCF, conv2_DCF, conv3_DCF, conv4_DCF, conv5_DCF]\r\n",
    "    convs = [None, vgg_m.conv1, vgg_m.conv2, vgg_m.conv3, vgg_m.conv4, vgg_m.conv5]\r\n",
    "    widths = [[width for i in range(6)]]\r\n",
    "    heights = [[height for i in range(6)]]\r\n",
    "    context_heights = [[context_height for i in range(6)]]\r\n",
    "    context_widths = [[context_width for i in range(6)]]\r\n",
    "    center_xs = [[center_x for i in range(6)]]\r\n",
    "    center_ys = [[center_y for i in range(6)]]\r\n",
    "\r\n",
    "    for i in range(1, cnt):\r\n",
    "        img_dir = os.path.join(imgs_dir, f'{i+1:08d}.png')\r\n",
    "        img = read_image(img_dir, ImageReadMode.RGB).to(dtype = torch.float32)\r\n",
    "        widths.append([])\r\n",
    "        heights.append([])\r\n",
    "        center_xs.append([])\r\n",
    "        center_ys.append([])\r\n",
    "        \r\n",
    "        for j in range(6):\r\n",
    "            center_x, center_y, width, height = center_xs[-2][j], center_ys[-2][j], widths[-2][j], heights[-2][j]\r\n",
    "            context_height, context_width = context_heights[-2][j], context_widths[-2][j]\r\n",
    "            cr_imgs = image.multiscales_cr(img, center_x, center_y, context_width,  context_height, scales, re_sz)\r\n",
    "            \r\n",
    "            for k in range(j+1):\r\n",
    "                if convs[k]:\r\n",
    "                    cr_imgs  = convs[k](cr_imgs)\r\n",
    "            \r\n",
    "            raw_score, scale_index, x, y = DCFS[j].forward(cr_imgs)\r\n",
    "            dis_x, dis_y = subgrid_detect(raw_score, context_width, context_height, x, y)\r\n",
    "            center_x += dis_x*scales[scale_index]\r\n",
    "            center_y += dis_y*scales[scale_index]\r\n",
    "            width *= scales[scale_index]\r\n",
    "            height *= scales[scale_index]\r\n",
    "            context_height *= scales[scale_index]\r\n",
    "            context_width *= scales[scale_index]\r\n",
    "            center_xs[-1].append(center_x)\r\n",
    "            center_ys[-1].append(center_y)\r\n",
    "            widths[-1].append(width)\r\n",
    "            heights[-1].append(height)\r\n",
    "\r\n",
    "            cr_img = image.cr(img, center_x, center_y, context_width, context_height, re_sz)\r\n",
    "            cr_img = torch.unsqueeze(cr_img, dim = 0)\r\n",
    "\r\n",
    "            for k in range(j + 1):\r\n",
    "                if convs[k]:\r\n",
    "                    cr_img = convs[k](cr_img)\r\n",
    "            DCFS[i].update(cr_img)\r\n",
    "            \r\n",
    "        image.show_imgs(img, center_x[-1], center_y[-1], widths[-1], heights[-1])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[2.],\n          [8.]]]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\r\n",
    "from torch import autograd\r\n",
    "\r\n",
    "trick = torch.tensor([[1., 2.]])\r\n",
    "\r\n",
    "\r\n",
    "def func(input):\r\n",
    "    input = input * input\r\n",
    "    output = torch.matmul(trick, input)\r\n",
    "    return output\r\n",
    "\r\n",
    "intt = torch.tensor([[1.], [2.]])\r\n",
    "autograd.functional.jacobian(func, intt).\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_scale_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3f1151fcd361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_scale_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '_scale_num' is not defined"
     ]
    }
   ],
   "source": [
    "from params import *\r\n",
    "_scale_num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "name": "python388jvsc74a57bd0241ed63ac3125269ccfabb3855d19e1ea73e07c2d25739720368db2e6952ce23"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}